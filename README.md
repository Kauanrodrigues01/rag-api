# ğŸ§  KnowMe RAG - Sistema de Respostas Inteligentes sobre VocÃª

<p align="center">
  <img src="URL_DA_IMAGEM_1" alt="Screenshot 1" width="700"/>
</p>

<p align="center">
  <img src="URL_DA_IMAGEM_2" alt="Screenshot 2" width="700"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI"/>
  <img src="https://img.shields.io/badge/LangChain-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white" alt="LangChain"/>
  <img src="https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white" alt="PostgreSQL"/>
  <img src="https://img.shields.io/badge/ChromaDB-FF6B6B?style=for-the-badge&logo=databricks&logoColor=white" alt="ChromaDB"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
  <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white" alt="OpenAI"/>
  <img src="https://img.shields.io/badge/Uvicorn-009688?style=for-the-badge&logo=gunicorn&logoColor=white" alt="Uvicorn"/>
  <img src="https://img.shields.io/badge/SQLAlchemy-D71F00?style=for-the-badge&logo=sqlalchemy&logoColor=white" alt="SQLAlchemy"/>
  <img src="https://img.shields.io/badge/Pytest-0A9EDC?style=for-the-badge&logo=pytest&logoColor=white" alt="Pytest"/>
</p>

---

## ğŸ“‹ Sobre o Projeto

**KnowMe RAG** Ã© uma API moderna e assÃ­ncrona desenvolvida com **FastAPI**, projetada para integrar dados pessoais (como currÃ­culo, projetos e experiÃªncias) em um pipeline **RAG (Retrieval-Augmented Generation)** com LLMs, utilizando **LangChain**, **ChromaDB**, **OpenAI Embeddings** e **Jinja2** para gerenciamento via interface web.

Com o **KnowMe RAG**, desenvolvedores podem fazer upload de arquivos PDF contendo dados sobre si mesmos e permitir que usuÃ¡rios faÃ§am perguntas e obtenham respostas contextuais baseadas nesses dados, como se estivessem conversando diretamente com vocÃª.

---

## âœ¨ Funcionalidades

* âœ… Upload de documentos PDF com extraÃ§Ã£o e divisÃ£o em *chunks*
* ğŸ” Busca semÃ¢ntica via embeddings + ChromaDB
* ğŸ¤– IntegraÃ§Ã£o com LLM (OpenAI GPT-3.5) para respostas contextuais via LangChain
* ğŸ§  TÃ©cnica de overlap nos chunks para manter o contexto
* ğŸ—ƒï¸ Banco de dados relacional (PostgreSQL) com SQLAlchemy para gerenciar documentos
* ğŸš® DeleÃ§Ã£o automÃ¡tica dos chunks ao excluir um documento
* ğŸ”’ ProteÃ§Ã£o de rotas via API Key (`X-API-KEY`)
* ğŸŒ Interface web usando Jinja2 (HTML, CSS, JS) para gerenciar documentos
* âš¡ Uso de `BackgroundTasks` do FastAPI para paralelizar operaÃ§Ãµes no vector store e melhorar performance (+70%)

---

## ğŸ§  Como funciona o RAG

1. **Upload de PDF**: O arquivo Ã© carregado, processado, dividido em chunks e enviado para a vector store.
2. **Armazenamento**: Os chunks sÃ£o salvos com identificadores Ãºnicos no ChromaDB e os metadados no PostgreSQL.
3. **Pergunta do usuÃ¡rio**: A pergunta Ã© embutida como vetor e comparada com os chunks via LangChain.
4. **Respostas contextuais**: Os melhores chunks sÃ£o combinados e enviados ao LLM, que retorna uma resposta rica em markdown.

---

## ğŸ› ï¸ Tecnologias Utilizadas

As principais tecnologias utilizadas neste projeto incluem:

* **FastAPI**: Framework web assÃ­ncrono para construÃ§Ã£o da API.
* **LangChain**: OrquestraÃ§Ã£o do pipeline RAG com LLM.
* **OpenAI**: Embeddings e modelo GPT-3.5 para geraÃ§Ã£o de respostas.
* **ChromaDB**: Armazenamento vetorial para busca semÃ¢ntica.
* **SQLAlchemy Async + PostgreSQL**: Banco relacional para metadados dos documentos.
* **Jinja2**: RenderizaÃ§Ã£o de templates para a interface de administraÃ§Ã£o.
* **Docker**: Empacotamento e deploy dos serviÃ§os.
* **Pytest**: Testes automatizados e cobertura.
* **Uvicorn**: Servidor ASGI de alta performance.

---

## ğŸš€ Executando Localmente com Docker

### ğŸ”§ PrÃ©-requisitos

* Docker + Docker Compose
* OpenAI API Key

### 1. Crie seu `.env`

Copie o arquivo `.env.example` e configure suas variÃ¡veis:

```bash
cp .env.example .env
```

Configure pelo menos as variÃ¡veis obrigatÃ³rias:

```env
# ObrigatÃ³rias
API_KEY=your-secret-api-key-here
OPENAI_API_KEY=sk-your-openai-api-key-here
DATABASE_URL=postgresql+asyncpg://user:pass@db:5432/dbname

# Opcionais (com valores padrÃ£o)
VECTOR_STORE_PATH=./chroma_db
LLM_MODEL=gpt-3.5-turbo
CHUNK_SIZE=1000
MAX_FILE_SIZE_MB=10
```

ğŸ“– **Para documentaÃ§Ã£o completa de todas as variÃ¡veis de ambiente, veja [CONFIGURATION.md](./CONFIGURATION.md)**

### 2. Suba os containers

```bash
docker compose up --build
```

### ğŸŒ Acesse:

* API Docs: [http://localhost:8000/docs](http://localhost:8000/docs)
* Health Check Simples: [http://localhost:8000/health](http://localhost:8000/health)
* Health Check Detalhado: [http://localhost:8000/health/detailed](http://localhost:8000/health/detailed)
* Painel Admin: [http://localhost:8000/admin](http://localhost:8000/admin)
* PGAdmin: [http://localhost:5050](http://localhost:5050)

---

## ğŸ“¦ Endpoints Principais

| MÃ©todo | Endpoint            | Protegido por API Key? | DescriÃ§Ã£o                                                       |
| ------ | ------------------- | ---------------------- | --------------------------------------------------------------- |
| POST   | `/documents`        | âœ…                      | Faz upload de 1 ou mais PDFs e envia chunks para o vector store |
| GET    | `/documents`        | âœ…                      | Lista os documentos salvos no banco                             |
| DELETE | `/documents/{id}`   | âœ…                      | Deleta o documento e seus chunks na vector store                |
| POST   | `/rag/ask-question` | âŒ                      | Faz uma pergunta com base nos documentos processados            |

**Para rotas protegidas, envie o header:**

```
X-API-KEY: <sua-chave>
```

---

## ğŸ”’ SeguranÃ§a

* Todas as rotas de CRUD de documentos sÃ£o protegidas por API Key.
* `get_api_key()` verifica o header `X-API-KEY` com um segredo definido no `.env`.
* ValidaÃ§Ã£o de tamanho mÃ¡ximo de arquivo (configurÃ¡vel via `MAX_FILE_SIZE_MB`).
* CORS configurÃ¡vel para ambientes de desenvolvimento e produÃ§Ã£o.
* Rodando com usuÃ¡rio nÃ£o privilegiado no Docker.
* SeparaÃ§Ã£o clara entre lÃ³gica de LLM e manipulaÃ§Ã£o de arquivos.
* Health checks detalhados para monitoramento de todos os componentes.

---

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

O projeto oferece ampla configurabilidade via variÃ¡veis de ambiente:

* **LLM**: Escolha o modelo (GPT-3.5/GPT-4), temperatura e tokens mÃ¡ximos
* **Embeddings**: Configure o modelo de embeddings da OpenAI
* **Chunking**: Ajuste tamanho e overlap dos chunks
* **CORS**: Configure origens permitidas para produÃ§Ã£o
* **Limites**: Defina tamanho mÃ¡ximo de arquivos
* **Health Checks**: Configure timeout dos health checks

ğŸ“– **Veja a [documentaÃ§Ã£o completa de configuraÃ§Ã£o](./CONFIGURATION.md) para todos os detalhes**

---

## ğŸ“Š MÃ©tricas de Performance

* UtilizaÃ§Ã£o de `BackgroundTasks` reduziu o tempo de resposta do endpoint de upload de documentos em \~70%
* Arquitetura desacoplada com ChromaDB e LangChain
* Docker com multi-stage build e imagens otimizadas

---

## ğŸ’¡ MotivaÃ§Ã£o

Esse projeto nasceu da necessidade de fornecer respostas automÃ¡ticas, seguras e contextualizadas para perguntas sobre o desenvolvedor (portfÃ³lio pessoal), transformando arquivos estÃ¡ticos em dados consultÃ¡veis via IA generativa.

---

## ğŸ§ª Futuras Melhorias

* ğŸ” Suporte a autenticaÃ§Ã£o OAuth2/JWT
* ğŸ§  Acompanhamento de perguntas e estatÃ­sticas
* ğŸŒ Deploy com HTTPS e CI/CD
* ğŸ–¼ï¸ Suporte a outros tipos de arquivo alÃ©m de PDF
* ğŸ“¦ Plugin para integraÃ§Ã£o com portfÃ³lios pÃºblicos

---

## ğŸ‘¨â€ğŸ’» Autor

**Kauan Rodrigues Lima**

* GitHub: [Kauanrodrigues01](https://github.com/Kauanrodrigues01)
* LinkedIn: [Kauan Rodrigues](https://www.linkedin.com/in/kauan-rodrigues-lima/)
